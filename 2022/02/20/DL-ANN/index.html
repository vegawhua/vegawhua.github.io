<!DOCTYPE html><html lang="en"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content=""><title>Deep Learning - ANN Part 1 | Vega's Blog</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="/css/dark.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/normalize.css/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/purecss/build/pure-min.min.css"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/purecss/build/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.jsdelivr.net/npm/jquery/dist/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script type="text/javascript" src="//cdn.jsdelivr.net/npm/clipboard/dist/clipboard.min.js"></script><script type="text/javascript" src="//cdn.jsdelivr.net/gh/codeseven/toastr/build/toastr.min.js"></script><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/codeseven/toastr/build/toastr.min.css"><div class="darkmode-toggle">ğŸŒ“</div><script>var prefersDarkMode = window.matchMedia('(prefers-color-scheme: dark)');
var toggle = document.querySelector('.darkmode-toggle');
var html = document.querySelector('html');

html.dataset.dark = localStorage.dark || prefersDarkMode.matches;

toggle.addEventListener('click', () => {
localStorage.dark = !(html.dataset.dark == 'true');
html.dataset.dark = localStorage.dark;
});</script><meta name="generator" content="Hexo 6.0.0"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Deep Learning - ANN Part 1</h1><a id="logo" href="/.">Vega's Blog</a><p class="description"></p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> Home</i></a><a href="/archives/"><i class="fa fa-archive"> Archive</i></a><a href="/about/"><i class="fa fa-user"> About</i></a><a href="/atom.xml"><i class="fa fa-rss"> RSS</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">Deep Learning - ANN Part 1</h1><div class="post-meta">2022-02-20<span> | </span><span class="category"><a href="/categories/DL/">DL</a><a href="/categories/DL/ANN/">ANN</a></span></div><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">Contents</div><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Basic-concepts"><span class="toc-number">1.</span> <span class="toc-text">Basic concepts</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#The-neuron"><span class="toc-number">1.1.</span> <span class="toc-text">The neuron</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#The-Activation-Function"><span class="toc-number">2.</span> <span class="toc-text">The Activation Function</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#How-do-Neural-Networks-work"><span class="toc-number">3.</span> <span class="toc-text">How do Neural Networks work?</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#How-do-Neural-Networks-learn"><span class="toc-number">4.</span> <span class="toc-text">How do Neural Networks learn?</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Gradient-Descent-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-number">5.</span> <span class="toc-text">Gradient Descent æ¢¯åº¦ä¸‹é™</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Stochastic-Gradient-Descent-%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-number">6.</span> <span class="toc-text">Stochastic Gradient Descent éšæœºæ¢¯åº¦ä¸‹é™</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Backpropagation-%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95"><span class="toc-number">7.</span> <span class="toc-text">Backpropagation åå‘ä¼ æ’­ç®—æ³•</span></a></li></ol></div></div><div class="post-content"><h2 id="Basic-concepts"><a href="#Basic-concepts" class="headerlink" title="Basic concepts"></a>Basic concepts</h2><h3 id="The-neuron"><a href="#The-neuron" class="headerlink" title="The neuron"></a>The neuron</h3><p>Components: Dendritesæ ‘çª â€“&gt; Neuronç¥ç»å…ƒ â€“&gt; Axonè½´çª</p>
<ul>
<li>Dendrites: signal receiver ä¿¡å·æ¥æ”¶</li>
<li>Axo: signal transmitter ä¿¡å·å‘é€</li>
</ul>
<p>âˆ´ ä¿¡å·ä¼ é€’çš„æœºåˆ¶ç§°ä¸º<strong>çªè§¦</strong> synapse - å°±æ˜¯ç¥ç»ç½‘ç»œé‡Œé‡Œè¿æ¥ç¥ç»å…ƒçš„<strong>çº¿</strong></p>
<hr>
<p>Input processing</p>
<p>âˆµ å€¼éƒ½<strong>ç±»ä¼¼</strong>ï¼Œ ç¥ç»ç½‘ç»œæ›´å¥½å¤„ç†ï¼Œæ¨¡å‹æ‰ä¼šæ›´å¥½</p>
<p>âˆ´ input value needs to be <strong>pre-processing</strong>:</p>
<ul>
<li><p><strong>standardize</strong> æ ‡å‡†åŒ– - average &#x3D; 0, varianceæ–¹å·® &#x3D; 1</p>
</li>
<li><p><strong>normalize</strong> å½’ä¸€åŒ– - åŸºäºæ•°æ®èŒƒå›´ï¼ŒæŠŠæ•°æ®è½¬åŒ–ä¸º0~1ä¹‹é—´çš„å€¼<br>$$<br>\frac{min} {max-min}<br>$$<br>ref. <a target="_blank" rel="noopener" href="http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf">Efficient BackProp</a></p>
</li>
<li><p>æ‰€æœ‰å€¼ä¼šè¢«<strong>ç´¯åŠ </strong>èµ·æ¥ or ä¹˜ä»¥<strong>æƒé‡å€¼</strong>å†ç´¯åŠ èµ·æ¥, etc.</p>
</li>
</ul>
<p>Output value can be:</p>
<ul>
<li>Continuous (price)</li>
<li>Binary (will exit yes&#x2F;no)</li>
<li>Categorical</li>
</ul>
<hr>
<p>Synapses</p>
<p>All synapses have <strong>weight</strong></p>
<p>Deciding <strong>Priority</strong>: <strong>Adjust the weight</strong> to decide which neuron is most important and which one is not.</p>
<p>å„ç§ç®—æ³•åŸºæœ¬å°±æ˜¯åœ¨è°ƒå‚</p>
<p>Signalsè¿›å…¥neuronåï¼š</p>
<ol>
<li>è®¡ç®—å¾—å‡ºåŠ æƒå’Œ</li>
<li>è¿ç”¨activation functionæ¿€æ´»å‡½æ•°</li>
<li>neuronæŠŠsignalæ²¿ç€synapse transfer to the next neuron</li>
</ol>
<h2 id="The-Activation-Function"><a href="#The-Activation-Function" class="headerlink" title="The Activation Function"></a>The Activation Function</h2><ol>
<li><p>Threshold Function é˜ˆå€¼å‡½æ•° - Y&#x2F;N function</p>
<p>â€‹    Ï•(x) &#x3D; 1 if x â‰¥ 0</p>
<p>â€‹    Ï•(x) &#x3D; 0 if x &lt; 0</p>
<p>Result: <code>Yes</code> or <code>No</code></p>
</li>
<li><p>Sigmoid Funtion<br>$$<br>\phi(x) &#x3D; \frac{1}{1+e^{-x}}<br>$$<br>xæ˜¯è¾“å…¥åŠ æƒå’Œ</p>
<p>â€‹    è¾“å…¥å€¼ &lt; 0ï¼Œ è¾“å‡ºè¶‹è¿‘äº0 â€“&gt; æ”¾å¼ƒè¿™ä¸ªè¾“å…¥</p>
<p>â€‹    è¾“å…¥å€¼ &gt; 0ï¼Œ è¾“å‡ºè¶‹è¿‘äº1 â€“&gt; é€šè¿‡è¿™ä¸ªè¾“å…¥</p>
<p>æ¯”é˜ˆå€¼å‡½æ•°Smoothï¼Œæ“…é•¿é¢„æµ‹å¯èƒ½æ€§ï¼ˆæ¦‚ç‡ï¼‰</p>
</li>
<li><p><strong>Rectifier</strong> Function æ•´æµå‡½æ•°</p>
<p>â€‹    è¾“å…¥å€¼ &lt; 0ï¼Œè¾“å‡º &#x3D; 0</p>
<p>â€‹    è¾“å…¥å€¼ â‰¥ 0ï¼Œè¾“å‡ºä¼šéšç€è¾“å…¥å€¼å¢åŠ è€Œ<strong>é€æ¸å¢åŠ </strong></p>
<p>ref. [<a target="_blank" rel="noopener" href="http://jmlr.org/proceedings/papers/v15/glorot11a/glorot11a.pdf">Deep sparse rectifier neural networks</a>]</p>
</li>
<li><p>Hyperbolic tangent Function åŒæ›²æ­£åˆ‡å‡½æ•° - å¾ˆåƒSigmoid<br>$$<br>\phi(x) &#x3D; \frac{1 - e^{-2x}}{1 + e^{-2x}}<br>$$</p>
</li>
</ol>
<p>å¸¸ç”¨<strong>é¢„æµ‹æ¦‚ç‡å€¼</strong>çš„ç»„åˆï¼š</p>
<p>Hidden layer â€“&gt; Rectifier</p>
<p>Output layer â€“&gt; Sigmoid</p>
<h2 id="How-do-Neural-Networks-work"><a href="#How-do-Neural-Networks-work" class="headerlink" title="How do Neural Networks work?"></a>How do Neural Networks work?</h2><p>æœ€åŸºæœ¬çš„ç¥ç»ç½‘ç»œç»“æ„ - å·²ç»å¯ä»¥å®Œæˆå¤§éƒ¨åˆ†MLç®—æ³•</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">graph LR</span><br><span class="line">    A(Area feet^2) --&gt;E(calculate)</span><br><span class="line">    B(Bedrooms) --&gt; E(calculate)</span><br><span class="line">    C(Distance to city miles) --&gt; E(calculate)</span><br><span class="line">    D(Age) --&gt; E(calculate)</span><br><span class="line">    E(calculate) --Price=w1*x1+w2*x2+w3*x3+w4*x4--&gt; F(price)</span><br></pre></td></tr></table></figure>

<p><strong>æ·»åŠ Hidden layerå¯ä»¥æé«˜NNé¢„æµ‹å‡†ç¡®ç‡ï¼Œå˜å¾—æ›´ä¸ºå¼ºå¤§</strong></p>
<ul>
<li>æ¯ä¸ªç¥ç»å…ƒè®¤ä¸ºé‡è¦çš„è¾“å…¥æ˜¯<em>ä¸åŒ</em>çš„ï¼Œä»¥æå–<em>ä¸åŒ</em>çš„<strong>è”ç³»</strong></li>
<li>æ¯ä¸ªç¥ç»å…ƒç»“åˆå¤šä¸ªå‚æ•°å¾—åˆ°ä¸€ä¸ªæ–°å‚æ•° - æ›´åŠ å‡†ç¡®</li>
<li>ç¥ç»å…ƒè¿˜å¯èƒ½å‘ç°æˆ‘ä»¬ä¸æ›¾å‘ç°çš„è‡ªå˜é‡ä¹‹é—´çš„<strong>æ–°å…³è”</strong></li>
</ul>
<p>âˆ´ Hidden layeræé«˜äº†NNçš„çµæ´»æ€§ï¼Œè®©NNå¯ä»¥å¯»æ‰¾é‚£äº›ç‰¹åˆ«çš„ç‰¹å¾å¹¶æŠŠä»–ä»¬ç»“åˆèµ·æ¥è€ƒè™‘ã€‚</p>
<h2 id="How-do-Neural-Networks-learn"><a href="#How-do-Neural-Networks-learn" class="headerlink" title="How do Neural Networks learn?"></a>How do Neural Networks learn?</h2><ol>
<li><p>hard-coded coding ç¡¬ç¼–ç  - å¸¸è§„SE</p>
<p>æŠŠç‰¹æ®Šè§„åˆ™å’Œæƒ³è¦çš„è¾“å‡ºç›´æ¥å‘Šè¯‰ç¨‹åºï¼Œå¸¦é¢†å®ƒçš„æ•´ä¸ªè¿è¡Œè¿‡ç¨‹ï¼Œå¹¶æŒ‡å®šç¨‹åºå¯èƒ½è¦å¤„ç†çš„å„ç§æƒ…å†µã€‚</p>
</li>
<li><p><strong>Mechanism</strong></p>
<p>å»ºç«‹ç¥ç»ç½‘ç»œï¼Œå‘Šè¯‰å®ƒå“ªä¸ªæ˜¯è¾“å…¥å“ªä¸ªæ˜¯è¾“å‡ºï¼Œç„¶åè®©æœºå™¨è‡ªå·±å»å¯»æ‰¾ç¡®å®šå…¶å®ƒçš„è§„åˆ™ã€‚</p>
</li>
</ol>
<p>Learning process - <strong>Backpropogationåå‘ä¼ æ’­ç®—æ³•</strong>:</p>
<ol>
<li>Compare Å· to y</li>
<li><strong>Loss fucntionæŸå¤±å‡½æ•°è®¡ç®—å·®å€¼</strong>  $C &#x3D; \frac{1}{2}(\hat{y}-y)^2$</li>
<li>å°†æŸå¤±å‡½æ•°çš„ç»“æœ<strong>åé¦ˆ</strong>ç»™NNï¼Œ</li>
<li>ä¸ºäº†<strong>æœ€å°åŒ–æŸå¤±å‡½æ•°</strong>ï¼Œå·®å€¼ç»“æœè¢«ä¼ é€’åˆ°äº†<strong>æƒé‡</strong>éƒ¨åˆ†ï¼Œæƒé‡å› æ­¤è¢«<strong>æ›´æ–°</strong></li>
</ol>
<p>ref. [<a target="_blank" rel="noopener" href="http://stats.stackexchange.com/questions/154879/a-list-of-cost-functions-used-in-neural-networks-alongside-applications">A list of cost functions used in neural networks, alongside applications</a>]</p>
<p>äº¤å‰éªŒè¯ &#x2F; æŸå¤±å‡½æ•°</p>
<hr>
<p>A basic NN example:</p>
<p>Feedforward neural networkå•å±‚å‰é¦ˆç¥ç»ç½‘ç»œ &#x3D; perceptronæ„ŸçŸ¥æœº ï¼ˆcr. Frank Rosenblatt)</p>
<p>åªæœ‰ä¸€å±‚hidden layer</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">graph LR</span><br><span class="line">    A(X1) --&gt;E(åŠ æƒæ±‚å’Œ)</span><br><span class="line">    B(X2) --&gt; E(åŠ æƒæ±‚å’Œ)</span><br><span class="line">    C(Xm) --&gt; E(åŠ æƒæ±‚å’Œ)</span><br><span class="line">    E(åŠ æƒæ±‚å’Œ) --activation functions--&gt; F(Å·)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>yæ˜¯çœŸå®ä¸–ç•Œçš„çœŸå®å€¼ï¼ŒÅ·ä»£è¡¨è¾“å‡ºå€¼ï¼Œæ˜¯ç”±NNå’Œç®—æ³•é¢„æµ‹çš„å€¼</p>
</blockquote>
<p>A multiple rows example:</p>
<blockquote>
<p>epoch:  <em>epochæ˜¯</em>ä¸€ä¸ªå•ä½ã€‚ ä¸€ä¸ªepochè¡¨ç¤ºå­¦ä¹ ä¸­<strong>æ‰€æœ‰</strong>è®­ç»ƒæ•°æ®å‡<strong>è¢«ä½¿ç”¨è¿‡ä¸€æ¬¡</strong>æ—¶çš„<strong>æ›´æ–°æ¬¡æ•°</strong>ã€‚</p>
</blockquote>
<p>æŸå¤±å‡½æ•°æ˜¯<strong>æ‰€æœ‰å·®å€¼</strong>çš„å¹³æ–¹å’Œçš„äºŒåˆ†ä¹‹ä¸€ $C &#x3D; \Sigma \frac{1}{2}(\hat{y}-y)^2$</p>
<h2 id="Gradient-Descent-æ¢¯åº¦ä¸‹é™"><a href="#Gradient-Descent-æ¢¯åº¦ä¸‹é™" class="headerlink" title="Gradient Descent æ¢¯åº¦ä¸‹é™"></a>Gradient Descent æ¢¯åº¦ä¸‹é™</h2><p>ç”¨é€”ï¼šæœ€å°åŒ–æŸå¤±å‡½æ•°</p>
<p>æ–¹æ³•ï¼š</p>
<ol>
<li><p>ç®€å•åŒ¹é…ç®—æ³•</p>
<p>ç”»ä¸ªå›¾å¾ˆå®¹æ˜“å‘ç°æœ€ä½ç‚¹ï¼Œä½†æ˜¯éšç€<strong>æƒé‡æ•°ç›®</strong>çš„å¢åŠ ï¼ŒNNé‡Œçš„<strong>çªè§¦æ•°ç›®</strong>ä¹Ÿä¼šå¢åŠ ï¼Œä¼šå¼•å‘ç»´æ•°ç¾éš¾<strong>Curse of Dimensionality</strong></p>
<p>e.g. 25ä¸ªweight â€“&gt; å°±ç®—å…¨ä¸–ç•Œæœ€å¿«çš„è®¡ç®—æœºï¼Œä¹Ÿè¦è®¡ç®—$3.42 \times 10^{50}$å¹´ï¼Œæ˜¯<strong>ç»å¯¹ä¸å¯èƒ½</strong>çš„ã€‚</p>
</li>
<li><p><strong>Gradient Descent</strong></p>
<ol>
<li><p>ä»æ›²çº¿ä¸Šçš„æŸä¸€ç‚¹å¼€å§‹</p>
</li>
<li><p>æ±‚å¯¼ï¼Œæ‰¾å‡ºé‚£ä¸€ç‚¹çš„æ–œç‡<strong>slope</strong>ï¼Œå¹¶ç¡®å®š<strong>æ­£è´Ÿ</strong>ï¼šè´Ÿçš„å¾€ä¸‹ï¼ˆå³ï¼‰èµ°ï¼Œæ­£çš„å¾€ä¸Šï¼ˆå·¦ï¼‰èµ°</p>
</li>
<li><p>é‡å¤è®¡ç®—ï¼Œç§»åŠ¨ç‚¹</p>
</li>
<li><p>ç›´åˆ°æ‰¾åˆ°æœ€ä¼˜æƒé‡</p>
<blockquote>
<p>Q: ä¸ºä»€ä¹ˆè¯è¶…ä¸‹èµ°ä¸ä¼šæœä¸Šèµ°ï¼Œä¸ä¼šè·³å‡ºæŸå¤±å‡½æ•°çš„æ›²çº¿ï¼Ÿ</p>
<p>A: ä¾èµ–äº<strong>å‚æ•°è°ƒèŠ‚</strong>ï¼Œä¹‹åä¼šè¯´åˆ°</p>
</blockquote>
</li>
</ol>
</li>
</ol>
<h2 id="Stochastic-Gradient-Descent-éšæœºæ¢¯åº¦ä¸‹é™"><a href="#Stochastic-Gradient-Descent-éšæœºæ¢¯åº¦ä¸‹é™" class="headerlink" title="Stochastic Gradient Descent éšæœºæ¢¯åº¦ä¸‹é™"></a>Stochastic Gradient Descent éšæœºæ¢¯åº¦ä¸‹é™</h2><p>éœ€è¦å®ƒçš„åŸå› ï¼š</p>
<p>æ¢¯åº¦ä¸‹é™è¦æ±‚æŸå¤±å‡½æ•°<strong>å¿…é¡»æ˜¯å‡¹å‡½æ•°</strong>ï¼Œå› æ­¤å¯ä»¥å¾—åˆ°æ­£ç¡®çš„æƒé‡ï¼›ä½†å½“æˆ‘ä»¬çš„æŸå¤±å‡½æ•°<strong>ä¸æ˜¯å‡¹å‡½æ•°</strong>æ—¶ï¼Œæ¯”å¦‚æˆ‘ä»¬é€‰æ‹©çš„æŸå¤±å‡½æ•°è™½ç„¶æ˜¯å‡¹å‡½æ•°ä½†æ˜¯åœ¨<strong>å¤šç»´ç©ºé—´</strong>é‡Œä»–ä¼šè½¬æ¢æˆ<strong>éå‡¹å‡½æ•°</strong>ã€‚è¿™ç§æƒ…å†µä¸‹æ‰¾åˆ°çš„æ˜¯æŸå¤±å‡½æ•°çš„ä¸€ä¸ª<strong>å±€éƒ¨æœ€å°å€¼</strong>ï¼Œè€Œä¸æ˜¯å…¨å±€æœ€å°å€¼ï¼Œä¼šå¾—åˆ°<strong>é”™è¯¯çš„æƒé‡</strong>ã€‚</p>
<p>æ‰€ä»¥è¿™ç§æ—¶å€™ï¼Œéœ€è¦<strong>éšæœºæ¢¯åº¦ä¸‹é™</strong>ã€‚</p>
<p>éšæœºæ¢¯åº¦ä¸‹é™<strong>é¿å…</strong>äº†<strong>é€‰ä¸­å±€éƒ¨æå€¼</strong>ã€‚</p>
<ul>
<li>å› ä¸ºå®ƒå¯ä»¥åŒ…å®¹è¿™ç§æ³¢åŠ¨ï¼Œå®ƒä¸€æ¬¡åªåšä¸€æ¬¡è¿­ä»£ï¼Œå› è€Œè®¡ç®—è¿‡ç¨‹ä¸­æ³¢åŠ¨å°±ä¼šå¤§å¾ˆå¤šï¼Œå°±æ›´æœ‰å¯èƒ½æ‰¾åˆ°å…¨å±€æœ€å°å€¼global minimumè€Œä¸æ˜¯å±€éƒ¨çš„ã€‚</li>
</ul>
<p>æ¢¯åº¦ä¸‹é™ vs. éšæœºæ¢¯åº¦ä¸‹é™</p>
<p><strong>å·¥ä½œæœºåˆ¶å·®å¼‚</strong></p>
<table>
<thead>
<tr>
<th align="left">æ¢¯åº¦ä¸‹é™</th>
<th>éšæœºæ¢¯åº¦ä¸‹é™</th>
</tr>
</thead>
<tbody><tr>
<td align="left">batchï¼Œè¿è¡Œäº†NNä¸Šæ‰€æœ‰è¡Œçš„æ•°æ®åå†è°ƒæ•´æƒé‡</td>
<td>è¿è¡Œä¸€è¡Œæ•°æ®å°±è°ƒæ•´å¯¹åº”çš„æƒé‡ï¼Œé‡å¤</td>
</tr>
<tr>
<td align="left">éœ€è¦æŠŠæ‰€æœ‰dataéƒ½ä¸Šè½½åˆ°ramé‡Œæ‰å¯ä»¥è¿è¡Œ</td>
<td>ä¸éœ€è¦æŠŠall dataéƒ½ä¸Šè½½åˆ°ramé‡Œå†è¿è¡Œï¼Œé€Ÿåº¦æ›´å¿«</td>
</tr>
<tr>
<td align="left">ç¡®å®šæ€§ç®—æ³•ï¼Œåªè¦NNæƒé‡çš„åˆå§‹å€¼ä¸€æ ·ï¼Œæ¯æ¬¡æ¢¯åº¦ä¸‹é™éƒ½ä¼šæœ‰ä¸€æ ·çš„ç»“æœ</td>
<td>éšæœºç®—æ³•ï¼Œå³ä½¿åˆå§‹å€¼ä¸€æ ·ï¼Œå› ä¸ºä¸åŒçš„è®­ç»ƒè¿‡ç¨‹ï¼Œæ¯æ¬¡éšæœºæ¢¯åº¦ä¸‹é™ç»“æœä¼šéšæœº</td>
</tr>
</tbody></table>
<hr>
<p>Mini batch gradient descent å°æ‰¹é‡æ¢¯åº¦ä¸‹é™</p>
<p>ç»“åˆnormalå’Œéšæœºçš„ä¼˜ç‚¹ï¼Œå¯ä»¥è‡ªå·±è®¾ç½®æ¯æ¬¡è¿è¡Œå¤šå°‘è¡Œæ•°åè°ƒæ•´æƒé‡</p>
<p>ref. </p>
<p>æ¢¯åº¦ä¸‹é™[<a target="_blank" rel="noopener" href="https://iamtrask.github.io/2015/07/27/python-network-part2/">A Neural Network in 13 lines of Python (Part 2 â€“ Gradient Descent)</a>]</p>
<p>ç®—æ³•çš„æ•°å­¦æœºåˆ¶ [<a target="_blank" rel="noopener" href="http://neuralnetworksanddeeplearning.com/chap2.html">Neural Networks and Deep Learning</a>]</p>
<h2 id="Backpropagation-åå‘ä¼ æ’­ç®—æ³•"><a href="#Backpropagation-åå‘ä¼ æ’­ç®—æ³•" class="headerlink" title="Backpropagation åå‘ä¼ æ’­ç®—æ³•"></a>Backpropagation åå‘ä¼ æ’­ç®—æ³•</h2><p>è®¡ç®—æŸå¤±å‡½æ•°åé¦ˆç»™NNçš„æƒé‡éƒ¨åˆ†è°ƒæ•´æƒé‡å°±æ˜¯Backpropagation</p>
<p>æ ¸å¿ƒ&amp;åŸºç¡€&amp;ä¼˜åŠ¿ï¼š</p>
<p>ç”±äºè¿™ä¸ªç®—æ³•çš„ç»“æ„ç‰¹ç‚¹ï¼Œèƒ½å¤Ÿä½¿æˆ‘ä»¬<strong>åŒæ—¶è°ƒæ•´æ‰€æœ‰çš„æƒé‡</strong>ï¼Œè¿™æ ·å°±èƒ½çŸ¥é“<strong>å“ªä¸€éƒ¨åˆ†è¯¯å·®æ˜¯ç”±NNçš„å“ªä¸ªæƒé‡å¼•èµ·çš„</strong>ã€‚</p>
<p>Training the ANN with Stochastic Gradient Descent</p>
<ol>
<li><p>Randomly initialise the weights to small numbers close to 0 (but not 0).</p>
<p>èµ‹äºˆæƒé‡æ¥è¿‘0ä½†ä¸æ˜¯0çš„å¾ˆå°çš„éšæœºæ•° - <strong>æƒé‡çš„åˆå§‹å€¼</strong></p>
</li>
<li><p>Input the first observation of your dataset in the input layer. Each feature is in one input node.</p>
<p>åœ¨è¾“å…¥å±‚ä¸­è¾“å…¥æ•°æ®é›†çš„ç¬¬ä¸€ä¸ªè§‚å¯Ÿå€¼ã€‚æ¯ä¸ªç‰¹å¾éƒ½åœ¨ä¸€ä¸ªè¾“å…¥èŠ‚ç‚¹ä¸­ã€‚</p>
</li>
<li><p>Forward-Propagationå‰å‘ä¼ æ’­: <strong>from left to right</strong>, the neurons are activated in a way that the impact of each neuronâ€™s activation is limited by the weights. Propagate the activations until getting the predicted result Å·.</p>
<p><strong>ä»å·¦åˆ°å³</strong>ï¼Œç¥ç»å…ƒè¢«æ¿€æ´»çš„æ–¹å¼æ˜¯æ¯ä¸ªç¥ç»å…ƒæ¿€æ´»çš„å½±å“å—åˆ°æƒé‡çš„é™åˆ¶ã€‚ä¼ æ’­æ¿€æ´»ç›´åˆ°å¾—åˆ°é¢„æµ‹ç»“æœ Å·ã€‚</p>
</li>
<li><p>Compare the predicted result Å· to the actual result y. Measure the generated error.</p>
<p>æ¯”è¾ƒé¢„æµ‹å€¼Å·å’ŒçœŸå®å€¼yï¼Œè®¡ç®—è¯¯å·®ã€‚</p>
</li>
<li><p><strong>Back-Propagation</strong>åå‘ä¼ æ’­: <strong>from right to left</strong>, the error is back-propagated. Update the weights according to how much they are responsible for the error. The <strong>learning rate</strong> decides by how much we update the weights.</p>
<p><strong>ä»å³åˆ°å·¦</strong>ï¼Œè¯¯å·®æ˜¯åå‘ä¼ æ’­çš„ã€‚æ ¹æ®ä»–ä»¬å¯¹é”™è¯¯è´Ÿè´£çš„ç¨‹åº¦æ›´æ–°æƒé‡ã€‚<strong>å­¦ä¹ ç‡</strong>å–å†³äºæˆ‘ä»¬<strong>å¯¹æƒé‡è¿›è¡Œäº†å¤šå°‘è°ƒæ•´</strong>ã€‚</p>
<blockquote>
<p>learning rateå­¦ä¹ ç‡å°±æ˜¯NNä¸­å¯ä»¥æ§åˆ¶çš„ä¸€ä¸ª<strong>å‚æ•°</strong></p>
</blockquote>
</li>
<li><p>Repeat Steps 1 to 5 and update the weights after each observation (<strong>Reinforcement Learning</strong>). Or: Repeat Steps 1 to 5 but update the weights only after a batch of observations (<strong>Batch Learning</strong>).</p>
<p>é‡å¤æ­¥éª¤ 1 åˆ° 5ï¼Œå¹¶åœ¨<strong>æ¯æ¬¡</strong>è§‚å¯Ÿåæ›´æ–°æƒé‡ï¼ˆ<strong>å¼ºåŒ–å­¦ä¹ </strong>ï¼‰</p>
<p>æˆ–è€…ï¼š</p>
<p>é‡å¤æ­¥éª¤ 1 åˆ° 5ï¼Œä½†<strong>ä»…åœ¨ä¸€æ‰¹</strong>è§‚å¯Ÿåæ›´æ–°æƒé‡ï¼ˆ<strong>æ‰¹é‡å­¦ä¹ </strong>ï¼‰</p>
</li>
<li><p>When the whole training set passed through the ANN, that makes an <strong>epoch</strong>. Redo more epochs.</p>
<p>å½“æ•´ä¸ªè®­ç»ƒé›†é€šè¿‡ ANN æ—¶ï¼Œå°±å½¢æˆäº†ä¸€ä¸ª <strong>epoch</strong>ã€‚ç»§ç»­é‡åšæ›´å¤šçš„<strong>epoch</strong>ã€‚</p>
</li>
</ol>
</div><div class="tags"><a href="/tags/DL/"><i class="fa fa-tag"></i>DL</a></div><div class="post-nav"><a class="next" href="/2022/02/20/DL/">Deep Learning Basic Concepts</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="http://example.com"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> Categories</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/DL/">DL</a><span class="category-list-count">2</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/DL/ANN/">ANN</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/Web-Development/">Web Development</a><span class="category-list-count">2</span></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> Tags</i></div><div class="tagcloud"><a href="/tags/Web/" style="font-size: 15px;">Web</a> <a href="/tags/CSS/" style="font-size: 15px;">CSS</a> <a href="/tags/DL/" style="font-size: 15px;">DL</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> Recent</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2022/02/20/DL-ANN/">Deep Learning - ANN Part 1</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/02/20/DL/">Deep Learning Basic Concepts</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/02/20/Bootstrap4%20Notes/">Bootstrap4 Notes</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/02/19/My-first-Blog/">My first Blog</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/02/19/CSS%20Note/">CSS Notes</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/02/19/hello-world/">Hello World</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> Links</i></div><ul></ul><a href="https://www.linkedin.com/in/vega-huang" title="LinkedIn" target="_blank">LinkedIn</a><ul></ul><a href="http://www.example2.com/" title="site-name2" target="_blank">site-name2</a><ul></ul><a href="http://www.example3.com/" title="site-name3" target="_blank">site-name3</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright Â© 2022 <a href="/." rel="nofollow">Vega's Blog.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="//cdn.jsdelivr.net/gh/fancyapps/fancybox/dist/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox/dist/jquery.fancybox.min.css"><script type="text/javascript" src="/js/copycode.js" successtext="Copy Successed!"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css"><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script></div></body></html>